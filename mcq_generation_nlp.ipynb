{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9163c21-9e5a-4843-a5fa-af7d38e638ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted tokens:\n",
      "['From', 'Trade', 'to', 'Territory', '                  \\n', 'The', 'Company', 'Establishes', 'Power', '\\n']\n",
      "Total number of paragraphs present:\n",
      "873\n",
      "Group 1 NER results:\n",
      "[('Trade to Territory                   \\n The Company Establishes Power 2', 'ORG'), ('Mughal', 'ORG'), ('India', 'GPE'), ('1707', 'DATE'), ('Mughal', 'ORG'), ('India', 'GPE'), ('Delhi', 'GPE'), ('the second half of the eighteenth century', 'DATE'), ('British', 'NORP'), ('British', 'NORP')]\n",
      "Group 2 NER results:\n",
      "[('Fig', 'PERSON'), ('1', 'CARDINAL'), ('Mughal', 'ORG'), ('Mughal', 'PERSON'), ('British', 'NORP'), ('1857', 'DATE'), ('Mughal', 'PERSON'), ('Bahadur Shah Zafar', 'PERSON')]\n",
      "Group 3 NER results:\n",
      "[('2023 - 24 10', 'DATE'), ('1600', 'DATE'), ('the East India Company', 'ORG'), ('England', 'GPE'), ('Queen Elizabeth', 'PERSON'), ('East', 'LOC'), ('England', 'GPE')]\n",
      "Group 4 NER results:\n",
      "[('the East India Company', 'ORG'), ('Company', 'ORG'), ('Europe', 'LOC'), ('Company', 'NORP'), ('English', 'LANGUAGE'), ('those days', 'DATE'), ('European', 'NORP'), ('Eastern', 'NORP'), ('first', 'ORDINAL'), ('English', 'LANGUAGE'), ('the west coast', 'LOC'), ('Africa', 'LOC'), ('the Cape of Good Hope', 'LOC')]\n",
      "Group 5 NER results:\n",
      "[('the Indian Ocean', 'LOC'), ('Portuguese', 'NORP'), ('India', 'GPE'), ('Goa', 'LOC'), ('Vasco da Gama', 'PERSON'), ('Portuguese', 'NORP'), ('India', 'GPE'), ('1498', 'DATE'), ('the early seventeenth century', 'DATE'), ('Dutch', 'NORP'), ('the Indian Ocean', 'LOC'), ('French', 'NORP'), ('India', 'GPE'), ('Europe', 'LOC'), ('European', 'NORP')]\n",
      "Group 6 NER results:\n",
      "[('seventeenth', 'ORDINAL'), ('eighteenth centuries', 'DATE'), ('Fig', 'PERSON'), ('2', 'CARDINAL'), ('Routes', 'GPE'), ('India', 'GPE'), ('the eighteenth century', 'DATE')]\n",
      "Group 7 NER results:\n",
      "[('2023 - 24', 'DATE'), ('11', 'CARDINAL'), ('East India Company', 'ORG'), ('Bengal', 'GPE'), ('first', 'ORDINAL'), ('English', 'LANGUAGE'), ('Hugli', 'LANGUAGE'), ('1651', 'DATE'), ('Company', 'ORG'), ('Company', 'ORG')]\n",
      "Group 8 NER results:\n",
      "[('1696', 'DATE'), ('Two years later', 'DATE'), ('Mughal', 'ORG'), ('Company', 'ORG'), ('three', 'CARDINAL'), ('One', 'CARDINAL'), ('Kalikata', 'PERSON'), ('Calcutta', 'ORG'), ('Kolkata', 'PERSON'), ('today', 'DATE'), ('Company', 'ORG'), ('Company', 'ORG'), ('Bengal', 'GPE')]\n",
      "Group 9 NER results:\n",
      "[('Murshid Quli Khan', 'PERSON'), ('Fig', 'PERSON'), ('3', 'CARDINAL'), ('Madras', 'LOC'), ('William Simpson', 'PERSON'), ('1867', 'DATE'), ('Farman', 'PERSON'), ('2023 - 24 12', 'DATE'), ('the early eighteenth century', 'DATE'), ('Company', 'ORG'), ('Bengal', 'GPE'), ('Aurangzeb', 'GPE'), ('Bengal', 'GPE'), ('Quli Khan', 'PERSON')]\n",
      "Group 10 NER results:\n",
      "[('Alivardi Khan', 'PERSON'), ('Sirajuddaulah', 'PERSON'), ('one', 'CARDINAL'), ('Company', 'NORP'), ('Company', 'ORG'), ('Company', 'ORG'), ('Bengal', 'GPE'), ('Company', 'ORG')]\n",
      "Contents of the first grouped paragraph:\n",
      "From Trade to Territory                   \n",
      " The Company Establishes Power 2 Aurangzeb was the last of the powerful Mughal rulers . He established control over a very large part of the territory that is now known as India . After his death in 1707 , many Mughal governors ( subadars ) and big zamindars began asserting their authority and establishing regional kingdoms . As powerful regional kingdoms emerged in various parts of India , Delhi could no longer function as an effective centre . By the second half of the eighteenth century , however , a new power was emerging on the political horizon â€“ the British . Did you know that the British originally came as a small trading company and were reluctant to acquire territories ? How then did they come\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    doc=fitz.open(pdf_path)#opens pdf at specified path\n",
    "    text=\"\"\n",
    "    for page_no in range(doc.page_count):#loops through all pages of pdf\n",
    "        page=doc[page_no]\n",
    "        text+=page.get_text()#stores all text content of pdf\n",
    "    doc.close()\n",
    "    return(text)\n",
    "def tokenize(text):\n",
    "    nlp=spacy.load(\"en_core_web_sm\")\n",
    "    doc=nlp(text)\n",
    "    tokens=[token.text for token in doc]#tokenizes extracted text\n",
    "    return(tokens)\n",
    "pdf_path='C:/Personal/ML/datasets/chapter-2.pdf'\n",
    "pdf_text=extract_text(pdf_path)#stores extracted text\n",
    "tokens=tokenize(pdf_text)#stores extracted tokens\n",
    "print('Extracted tokens:')\n",
    "print(tokens[:10])\n",
    "\n",
    "def paragraphs_org(tokens):\n",
    "    paragraphs = []\n",
    "    current_paragraph = []\n",
    "    \n",
    "    for i,token in enumerate(tokens):#loops through tokens\n",
    "        if token == '\\n':#on detection of newline character:\n",
    "            if current_paragraph:\n",
    "                paragraphs.append({'paragraph': current_paragraph})\n",
    "                current_paragraph = []\n",
    "            else:\n",
    "                current_paragraph.append(token)#appended to present paragraph if does not satify condition\n",
    "        else:\n",
    "            current_paragraph.append(token)#appended to present paragraph if does not satify condition\n",
    "\n",
    "    if current_paragraph:\n",
    "        paragraphs.append({'paragraph': current_paragraph})#individual paragraphs appended to paragraphs dictionary\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "paragraphs=paragraphs_org(tokens)\n",
    "print('Total number of paragraphs present:')\n",
    "print(len(paragraphs))\n",
    "\n",
    "documents=[' '.join(p['paragraph']) for p in paragraphs]\n",
    "g_paragraphs=[' '.join(documents[i:i+15]) for i in range(0, len(documents),15)]#grouped 15 extracted paragraphs together\n",
    "\n",
    "def ner(grouped_paragraphs):#named entity recognition function\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    ner_results = []\n",
    "    for i, group in enumerate(grouped_paragraphs):\n",
    "        doc = nlp(group)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]#helps extract the entities and their corresponding labels\n",
    "        ner_results.append({'group_number':i+1,'n_entities': entities})\n",
    "    return ner_results\n",
    "\n",
    "grouped_ner_results=ner(g_paragraphs)#stores entities of compiled paragraphs\n",
    "\n",
    "for result in grouped_ner_results[:10]:\n",
    "    print(f'Group {result[\"group_number\"]} NER results:')\n",
    "    print(result['n_entities'])\n",
    "\n",
    "print('Contents of the first grouped paragraph:')\n",
    "print(g_paragraphs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d127877a-f229-4e7a-8f4b-97198397cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords of group 1\n",
      "{'group_number': 1, 'keywords': ['the', 'regional', 'as', 'powerful', 'kingdoms']}\n",
      "Keywords of group 2\n",
      "{'group_number': 2, 'keywords': ['bahadur', 'shah', 'zafar', 'mughal', 'the']}\n",
      "Keywords of group 3\n",
      "{'group_number': 3, 'keywords': ['east', 'england', 'the', 'cold', 'compete']}\n",
      "Keywords of group 4\n",
      "{'group_number': 4, 'keywords': ['could', 'the', 'charter', 'sell', 'cheap']}\n",
      "Keywords of group 5\n",
      "{'group_number': 5, 'keywords': ['the', 'in', 'ocean', 'portuguese', 'too']}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#uses TF IDF method for keyword extraction\n",
    "def extract_keywords(grouped_paragraphs):\n",
    "    vectorizer=TfidfVectorizer()#instance of vectorizer\n",
    "    tf_idf_matrix=vectorizer.fit_transform(grouped_paragraphs)#calculates a TF IDF matrix with each row correspodning to a paragraph\n",
    "    #each column corresponds to a word\n",
    "    feature_names=vectorizer.get_feature_names_out()\n",
    "    keywords=[]\n",
    "\n",
    "    for i, group in enumerate(grouped_paragraphs):\n",
    "        tf_idf_scores=tf_idf_matrix[i].toarray().flatten()\n",
    "        top_indices=tf_idf_scores.argsort()[-5:][::-1]\n",
    "        top_keywords=[feature_names[index] for index in top_indices]\n",
    "        keywords.append({'group_number':i+1,'keywords': top_keywords})#keywords stored in a dictionary\n",
    "\n",
    "    return keywords\n",
    "\n",
    "keywords=extract_keywords(g_paragraphs)\n",
    "for i,p in enumerate(keywords[:5]):\n",
    "    print(f'Keywords of group {i+1}')\n",
    "    print(p)#sample keywords printed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "748d875f-660e-41f6-90f2-7a47a61c05f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mughal officials into giving the Company zamindari rights1696 (DATE)Mughal (ORG)One (CARDINAL)Kalikata (PERSON)', 'Mughal emperor Aurangzeb1696 (DATE)Mughal (ORG)One (CARDINAL)Kalikata (PERSON)', 'Mughal officials1696 (DATE)Mughal (ORG)One (CARDINAL)Kalikata (PERSON)', 'Bengal1696 (DATE)Mughal (ORG)One (CARDINAL)Kalikata (PERSON)', 'Bengal.1696 (DATE)Mughal (ORG)One (CARDINAL)Kalikata (PERSON)']\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "#T5 LLM used here\n",
    "import random\n",
    "def generate_mcq(context,model,tokenizer,num_choices=4): #generates mcq questions\n",
    "    text=f\"context: {context} generate multiple choice question:\" #serves as a guide to the LLM\n",
    "\n",
    "    encoding=tokenizer.encode_plus(text,max_length=384,pad_to_max_length=False,truncation=True,return_tensors=\"tf\")#converts text to numerical format\n",
    "    #uses tokenizer to obtain input IDs etc\n",
    "    input_ids,attention_mask=encoding[\"input_ids\"],encoding[\"attention_mask\"]\n",
    "    #attention mask sieves out relevant and useful tokens\n",
    "\n",
    "    outs=model.generate(input_ids=input_ids,attention_mask=attention_mask,early_stopping=True,num_beams=5,\n",
    "        num_return_sequences=5,no_repeat_ngram_size=2,max_length=100)\n",
    "    #above snippet generates question 5 possibilities considered parallely, early stopping enabled\n",
    "    \n",
    "    dec=[tokenizer.decode(ids, skip_special_tokens=True) for ids in outs] #helps decode the outputted IDs into text\n",
    "    generated_questions=[]\n",
    "    for i in range(5):\n",
    "        generated_questions.append(dec[i].replace(\"question:\", \"\").strip()) #only 1 sequence outputted so we use dec[0]\n",
    "\n",
    "    nlp=spacy.load(\"en_core_web_sm\")\n",
    "    doc=nlp(context)\n",
    "    entities=set([(ent.text, ent.label_) for ent in doc.ents]) #extracts relevant important entities from context\n",
    "    #this helps in generating choicws\n",
    "    #keys=[]\n",
    "    #for k in keywords:\n",
    "        #for x in k['keywords']:\n",
    "            #keys.append(x)\n",
    "\n",
    "    #combined_entities=entities.union(set(keywords))#combines extracted entities and keywords together\n",
    "    #shuffled_entities=list(combined_entities)\n",
    "    #random.shuffle(shuffled_entities)\n",
    "    entities=list(entities)\n",
    "    answer_choices=[f\"{ent[0]} ({ent[1]})\" for ent in entities[:num_choices]]#generates answer choices on basis of extracted entities\n",
    "\n",
    "    return generated_questions,answer_choices\n",
    "\n",
    "\n",
    "\n",
    "def get_mca_questions(context):\n",
    "    if not isinstance(context,str):\n",
    "        raise TypeError(\"Received parameter must be a string\")\n",
    "    model_name = \"t5-small\"\n",
    "    model=TFT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer=T5Tokenizer.from_pretrained(model_name)#T5 small model and tokenizers initialized\n",
    "    \n",
    "    generated_questions,answer_choices=generate_mcq(context,model,tokenizer)\n",
    "    mca_questions=[]\n",
    "    answers=\"\"\n",
    "    for i in answer_choices:\n",
    "        answers+=i\n",
    "    for i in generated_questions:\n",
    "        x=i+answers\n",
    "        mca_questions.append(x)\n",
    "    return(mca_questions)\n",
    "#we can pass passages present in g_paragraphs as context parameters to above function, it would return mcq questions corresponding \n",
    "#to that particular context\n",
    "\n",
    "ans=get_mca_questions(g_paragraphs[7])\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c58a1-cef5-404b-8c4d-60bec1efd86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4471a-2474-4485-bb7e-bc56425ed6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
